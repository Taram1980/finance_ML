{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\afecn\\anaconda3.1\\lib\\importlib\\_bootstrap.py:219: RuntimeWarning: numpy.ufunc size changed, may indicate binary incompatibility. Expected 192 from C header, got 216 from PyObject\n",
      "  return f(*args, **kwds)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'metrics(pp,x_test_i,x_train_i,x_cv_i, thresh, k, verbose)'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#my_func\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, roc_auc_score, cohen_kappa_score, accuracy_score\n",
    "\n",
    "def metrics(pp,x_test_i,x_train_i,x_cv_i, thresh, k, verbose):\n",
    "    model = load_model('.' + str(pp))\n",
    "    #МЕТРИКА ЭВАЛЮЕЙТ\n",
    "    test_res = model.evaluate(x_test_i, y_test, verbose=verbose)\n",
    "    print(\"keras evaluate test=\", test_res)\n",
    "    train_res = model.evaluate(x_train_i, y_train, verbose=verbose)\n",
    "    print(\"keras evaluate train=\", train_res)\n",
    "    #СПРОГНОЗИРОВАЛ\n",
    "    pred       = model.predict(x_test_i)\n",
    "    pred_train = model.predict(x_train_i)\n",
    "    pred_cv    = model.predict(x_cv_i)\n",
    "    #ПО КЛАССАМ\n",
    "    pred_classes       = np.argmax(pred, axis=1)\n",
    "    pred_classes_train = np.argmax(pred_train, axis=1)\n",
    "    pred_classes_cv    = np.argmax(pred_cv, axis=1)\n",
    "    #пПО КЛАССАМ ЛЕЙБЛЫ\n",
    "    y_test_classes  = np.argmax(y_test, axis=1)\n",
    "    y_train_classes = np.argmax(y_train, axis=1)\n",
    "    y_cv_classes    = np.argmax(y_cv, axis=1)\n",
    "    #check_baseline(pred_classes, y_test_classes)\n",
    "\n",
    "    #МАТРИЦЫ СМУЩЕНЫЕ\n",
    "#    labels = [0,1,2]\n",
    "    conf_mat  = confusion_matrix(y_test_classes,  pred_classes)\n",
    "    c_m_train = confusion_matrix(y_train_classes, pred_classes_train)\n",
    "    c_m_cv    = confusion_matrix(y_cv_classes,    pred_classes_cv)\n",
    "    print('test')\n",
    "    print(conf_mat)\n",
    "    print('train')\n",
    "    print(c_m_train)\n",
    "    print('cv')\n",
    "    print(c_m_cv)\n",
    "\n",
    "    f1_weighted = f1_score(y_test_classes, pred_classes, labels=None, \n",
    "             average='weighted', sample_weight=None)\n",
    "    print(\"F1 score (weighted)\", f1_weighted)\n",
    "    print(\"F1 score (macro)\", f1_score(y_test_classes, pred_classes, labels=None, \n",
    "             average='macro', sample_weight=None))\n",
    "    print(\"F1 score (micro)\", f1_score(y_test_classes, pred_classes, labels=None, \n",
    "             average='micro', sample_weight=None))  # weighted and micro preferred in case of imbalance\n",
    "\n",
    "    # https://scikit-learn.org/stable/modules/model_evaluation.html#cohen-s-kappa --> supports multiclass; ref: https://stats.stackexchange.com/questions/82162/cohens-kappa-in-plain-english\n",
    "    print(\"cohen's Kappa\", cohen_kappa_score(y_test_classes, pred_classes))\n",
    "\n",
    "    recall = []\n",
    "    for i, row in enumerate(conf_mat):\n",
    "        recall.append(np.round(row[i]/np.sum(row), 2))\n",
    "        print(\"Recall of class {} = {}\".format(i, recall[i]))\n",
    "    print(\"Recall avg\", sum(recall)/len(recall))    \n",
    "    print('end')\n",
    "    #Finance\n",
    "    \n",
    "#     y1 = [pred, pred_train,pred_cv] #прогонозы\n",
    "#     x1 = [y_test,y_train,y_cv] #лейблы\n",
    "#     z1 = [Pr_test,Pr_train,Pr_cv] #прогноp\n",
    "#     q1 = ['test','train','cv'] #тексты\n",
    "\n",
    "#     aa_all=pd.DataFrame()\n",
    "#     bb_all=pd.DataFrame()\n",
    "\n",
    "\n",
    "#     for (y,x,z,q) in zip(y1,x1,z1,q1):\n",
    "\n",
    "#         print(q)\n",
    "#         result1= pd.DataFrame()\n",
    "#         result1 = pd.concat([pd.DataFrame(y), pd.DataFrame(np.argmax(x, axis=1)), pd.DataFrame(np.array(z))], axis=1) ##БЕРЕМ ПРОГНОЗ 2* БЕРЕМ МЕТКУ 1* И %\n",
    "#         result1.columns = ['sell_f', 'buy_f', 'label', 'return_']\n",
    "#         result1['result'] = np.where(np.argmax(y, axis=1)==result1['label'], 1, 0)\n",
    "#         display(result1['result'].mean())\n",
    "\n",
    "#             #ПОРОГ ДЛЯ УВЕРЕННОСТЬ ПРОГНОЗА\n",
    "#         result1['forec50'] = np.where(result1['buy_f']>thresh, 1, np.where(result1['sell_f']>thresh,0,'no opinion'))\n",
    "#             #БЕЗ ПОРОГА ДЛЯ УВЕРЕННОСТЬ ПРОГНОЗА\n",
    "#         result1['forec']   = np.argmax(y, axis=1)\n",
    "#             #УБРАЛ% -99.99\n",
    "#         result2 = result1[(result1['return_']<=k)&(result1['return_']>=-k)]\n",
    "#             #СВОДНАЯ\n",
    "#         aa = pd.pivot_table(result2, index='forec', values='return_', aggfunc=['count', 'mean'], margins=True)\n",
    "#         aa_all = pd.concat([aa_all, aa], axis=1)\n",
    "\n",
    "#             #С ПОРОГОМ\n",
    "#         bb = pd.pivot_table(result2, index='forec50', values='return_', aggfunc=['count', 'mean'], margins=True)\n",
    "#         bb_all = pd.concat([bb_all, bb], axis=1)\n",
    "\n",
    "#     aa_all.columns = ['count_test', 'Pr_test','count_train', 'Pr_train','count_cv', 'Pr_cv',]    \n",
    "#     display(aa_all)\n",
    "#     print(10*'****')\n",
    "#     print('thresh = ' + str(thresh))\n",
    "#     bb_all.columns = ['count_test', 'Pr_test','count_train', 'Pr_train','count_cv', 'Pr_cv',] \n",
    "#     display(bb_all)\n",
    "    \n",
    "display('metrics(pp,x_test_i,x_train_i,x_cv_i, thresh, k, verbose)')        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'accuracy' from 'sklearn.metrics' (C:\\Users\\afecn\\anaconda3.1\\lib\\site-packages\\sklearn\\metrics\\__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-20f38162b912>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mconfusion_matrix\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mroc_auc_score\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcohen_kappa_score\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maccuracy\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'accuracy' from 'sklearn.metrics' (C:\\Users\\afecn\\anaconda3.1\\lib\\site-packages\\sklearn\\metrics\\__init__.py)"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
